---
title: "Literature Review"
author: "Susan Vanderplas"
date: "7/2/2020"
output: html_document
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
library(tidyverse)
```

## Papers

- @wagenaarMisperceptionExponentialGrowth1975
    - Numerical estimation is more accurate than graphical estimation for exponential curves
    - Perceptual issue: ability to discriminate between values of $\beta$ in functions $y = \exp{\beta x}$
    - Aspect ratio of the graph didn't significantly affect misperception
    - Instruction on exponential growth *reduces* underestimation, but doesn't change issues with consistent underestimation of $\beta$ - correction factor is the same across all problems. That is, participants increased estimates of $y = \alpha \exp{\beta x}$ by increasing $\alpha$, not by changing $\beta$. Accuracy was still not as good using graphs as with numerical estimation
    - No correlation of accuracy with mathematical credit hours found (Experiment 1)
  
  - Takeaway: Using a log scale should make it much easier to estimate $\beta$ - we estimate slopes relatively accurately [@mosteller_eye_1981], resolving much of the difficulty with exponential estimation
  - Takeaway 2: Direction of error (underestimation) is the same regardless of the question type asked:
      - Predict for (t+5)
      - Predict for (t+1, t+2, ..., t+5)
      - At what time t will y surpass (value)? (Inverse prediction)
    That is, in the 3rd formulation, t is consistently over-estimated (corresponding to an under-estimation of y at time t)

- @timmersInverseStatisticsMisperception1977a
    - Compared graphical and numerical estimation of decreasing exponential ($y = e^{-x}$) curves
    - Numerical estimation:
        - make a prediction for 1980
        - When will the process reach (y) (always targeted at 1980)
    - Graphical task: presented with an increasing and decreasing exponential series, which of the two series will reach y = ... first?
    
    - I don't fully understand the methods in this paper, particularly the graphical scale interlocking and rearrangement. It seems a bit insane. 
    - Plotting exponential growth inversely reduces misperception -- likely because the steepest part of the curve is shown early in the curve when participants are creating a heuristic for prediction. 
    
```{r, include = -1}
combinations <- bind_rows(
tibble(a = 1000, b = seq(-.1, -.7, by = -.1)), 
tibble(a = 25000, b = seq(-.3, -.9, by = -.1)),
tibble(a = 625000, b = seq(-0.5, -1.1, by = -.1))
)

data <- combinations %>%
  mutate(id = 1:n(), 
         data = purrr::map2(a, b, ~tibble(x = 0:4, y = .x * exp(.y * 0:4)))) %>%
  unnest(data)

## Numerical estimation
ggplot(data, aes(x = x+1971, y = y)) + geom_line() + 
  facet_wrap(~a + b, scales = "free", nrow = 3, labeller = label_both) +
  scale_x_continuous(breaks = 1971:1976, labels = c("71", "72", "73", "74", "75", "76")) + 
  theme(axis.text.y = element_blank()) + 
  ggtitle("Timmers & Wagenaar 1978, numerical estimation")

f1 <- function(b) function(x) exp(b * (x - 9))
f2 <- function(b) function(x) exp(-b * x) - exp(9 * -b)
f3 <- function(b) function(x) 1 - exp(-b*x) + exp(9 * -b)
f4 <- function(b) function(x) 1 - exp(b * (x - 9))

fs <- tibble(fun = list(f1, f2, f3, f4), series = c("A", "A", "B", "B"), line = c(1, 2, 1, 2)) %>%
  crossing(b = seq(.1, .6, .1)) %>%
  mutate(actual_fun = map2(fun, b, ~.x(.y))) %>%
  select(-fun) %>%
  pivot_wider(names_from = line, names_prefix = "line", values_from = actual_fun)

sets <- crossing(select(fs, series1 = series, b1 = b, line1), select(fs, series2 = series, b2 = b, line2)) %>%
  filter(series1 == series2) %>%
  group_by(series1) %>%
  mutate(id = 1:n()) %>%
  select(-series2) %>%
  rename(series = series1) %>%
  group_by(id) %>%
  nest(line1 = c(b1, line1), line2 = c(b2, line2)) %>%
  mutate(across(matches("line"), ~map(., set_names, nm = c("b", "line")))) %>%
  pivot_longer(line1:line2, names_to = "tmp", values_to = "data") %>%
  unnest(c(data)) %>%
  mutate(data = purrr::map(line, ~tibble(x = seq(0, 16.7, .1), y = .(x)))) %>%
  unnest(data)

sets_label <- select(sets, series, id, b, tmp) %>%
  unique() %>%
  pivot_wider(names_from = tmp, values_from = b) %>%
  mutate(label = sprintf("b1 = %.1f, b2 = %.1f", line1, line2)) %>%
  select(series, id, label)

sets <- left_join(sets, sets_label, by = c("series", "id"))

filter(sets, series == "A", x < 9) %>%
  ggplot(aes(x = x, y = y, group = tmp)) + geom_line() + facet_wrap(~label) + 
  coord_cartesian(ylim = c(0, 1), xlim = c(0, 16.7)) + 
  scale_y_continuous(breaks = c(0, 1)) + 
  ggtitle("Timmers & Wagenaar 1978, visual estimation, Set A")

filter(sets, series == "B", x < 9) %>%
  ggplot(aes(x = x, y = y, group = tmp)) + geom_line() + facet_wrap(~label) + 
  coord_cartesian(ylim = c(0, 1), xlim = c(0, 16.7)) + 
  scale_y_continuous(breaks = c(0, 1)) + 
  ggtitle("Timmers & Wagenaar 1978, visual estimation, Set B")
```

- @goodwin1994heuristics
    - Overview of what is known about forecasting of time series data
    - Different task components - first, understand the process producing the data, then use simplifying assumptions to forecast (ignoring the larger process)
    - "Anchoring and adjustment strategies"
        - stationary TS: use long term mean of the series as anchor, last observation $F_{t+1} = \overline Y + \alpha(Y_t - \overline Y)$ [@lawrence1992exploring]
        - TS with trend: heuristic similar to linear trend exponential smoothing, with two estimation parameters - uses two anchor/adjustment heuristics - one for trend, one for previous forecast. Much greater cognitive effort, and possibility of a trend toward underestimates when slope is positive and overestimates when the slope is negative. May also depend on trend salience and latest observation spacing. 
        - TS with noise: noise disproportionately affects forecasts [@oconnorJudgementalForecastingTimes1993] as does the complexity of the underlying signal (seasonality, trend, autocorrelation)
    - Optimum number of data points - studies with exponential curves [@wagenaar_extrapolation_1978] showed greater underestimation with more data points, but the design is problematic, because the "anchoring points" of the two compared series are different, leading to different slopes. (see the pink and blue lines in the image below vs. the underlying exponential curve)
```{r}
tibble(x = c(1:5), y = c(3, 7, 20, 55, 148), s2 = 1:5 %% 2) %>%
  bind_rows(mutate(., s2 = 0)) %>% unique() %>%
  mutate(series = paste("series", 1+s2)) %>%
  ggplot(aes(x = x, y = y, color = series)) + 
  geom_function(fun = exp, inherit.aes = F, color = "grey", alpha = .75) + 
  geom_point(size = 3) + 
  geom_line() + 
  # facet_wrap(~series) + 
  guides(color = F) + 
  theme_bw() + 
  xlim(c(1, 7))
```
    - Decomposing the extrapolation task makes predictions more accurate. e.g. first predicting based on the trend, then the seasonal component, then the random component, and adding the predictions together. This could be a result of e.g. sine illusion type effects (removing the mean makes variability estimation *much* easier) as well as just based on reduction in task complexity. 
    - Contextual information is important, both because it increases motivation and because it provides some information about likely future values in the presence of seasonal fluctuations.
    - List of biases that matter:
        - anchoring biases
        - optimism biases
        - illusory correlation
        - recency bias
        - selective perception
        - inconsistency in judgement


- @best2007perception
    - Compared many factors:
        - exponential, asymptotic, and linear trends
        - increasing or decreasing
        - bar, suspended bar, scatter, and line plots
        - number of points
        - high, medium, low variability
    - Asked to identify the type of curve (exponential, asymptotic, linear; increasing, decreasing)
        - definition of exponential decreasing is ... not what a mathematician would use. $y = C + -e^x$ instead of $y = e^{-x}$. $y=e^{-x}$ is what they've defined as asymptotic decreasing.
    - n=6 with ~2k trials per participant
    - hypothesis is 2-stage estimation: first, identify the type of curve and direction, then use that information for prediction
        - this experiment is examining whether discrimination between curve types is possible
    - accuracy higher when nonlinear trends presented (e.g. it's hard to say something is linear, but easy to say that it isn't)
    - accuracy higher with low variability -- variability was additive, e.g. constant variance around mean function
    - it appears that participants examined curvature to make the determination of type
    
  - Takeaway: Use line/scatter plots
  - Takeaway 2: Increasing # points helps with determination of linearity but doesn't affect determination of nonlinearity
  - Takeaway 3: Accuracy ranged from 75% to 50% (ish) at determination of the type of curve. Any prediction errors would be on top of that. 

## Possible Factors
### Things that matter
- variability [@best2007perception] - low variability = higher accuracy

- sample size
  - for linear trends, more points increase accuracy [@best2007perception; @lewandowsky_perception_1989]
  - for asymptotics and exponentials, more points doesn't change accuracy [@best2007perception] (when asking about the type of curve -- not numerical estimation)
  - for exponentials, more points decreases accuracy [@wagenaar1979pond; @wagenaar_extrapolation_1978]

- plot form [@best2007perception] - scatterplots and line graphs are more accurate (but not mutally distinguishable, e.g. adding the trend line didn't significantly change the results) -- when asking about the type of curve

- How much of the curve shape is shown early on e.g. raw change in y is hypothesized as the explanation for why the negative curves are better estimated than the positive curves in @timmersInverseStatisticsMisperception1977a


### Things that don't matter
- aspect ratio [@wagenaarMisperceptionExponentialGrowth1975]

## References
