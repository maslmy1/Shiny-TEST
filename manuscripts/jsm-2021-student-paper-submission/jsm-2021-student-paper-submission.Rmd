---
title: |
  Perception of exponentially increasing data displayed on a log scale
type: 
  - JSM 2021 Student Paper Award (ASA sections on Statistical Computing and Statistical Graphics)
author:
  - name: Emily A. Robinson
    affil: a
    email: emily.robinson@huskers.unl.edu
  - name: Reka Howard
    affil: a
    email: rekahoward@unl.edu
  - name: Susan VanderPlas
    affil: a
    email: susan.vanderplas@unl.edu
affiliation:
  - num: a
    address: |
      Department of Statistics, University of Nebraska - Lincoln,
bibliography: references.bib
# appendix: appendix.tex
abstract: |
  Log scales are often used to display data over several orders of magnitude within one graph. During the COVID pandemic, we’ve seen both the benefits and the pitfalls of using log scales to display data. This paper aims to...
keywords: |
  Exponential; Log; Visual Inference; Perception
header-includes: |
  \usepackage{hyperref}
  \usepackage[utf8]{inputenc}
  \def\tightlist{}
  \usepackage[usenames,dvipsnames]{color}
  \newcommand{\er}[1]{\textcolor{Orange}{#1}}
  \newcommand{\svp}[1]{\textcolor{Green}{#1}}
  \newcommand{\rh}[1]{\textcolor{Plum}{#1}}
output: rticles::tf_article
---

<!-- How to comment out -->

<!-- 
\svp{Emily and Reka, when we get into the editing stage, I've found this strategy to be useful: basically, when you add new text, use your color (feel free to change the command, for now yours are set to \er{$\backslash$er} and \rh{$\backslash$rh}.}
\svp{The way this usually works is that when e.g. I read over a document that Emily has recently edited, I will remove her flagged text to indicate that I've seen/accepted the changes (and vice versa - I'll edit text and highlight it with my color, and you can accept/modify and flag yours too) -- sometimes modifications happen first and then all of the color in a paragraph gets taken out once we've moved on.}
\svp{This not only leads to a nice rainbow effect, but you can quickly spot changes, too. If you're changing some slight phrasing/wording that doesn't change meaning, it's not necessary to highlight those changes - highlight content changes, not e.g. verb tenses.}
\svp{If something is a comment and has been addressed, comment it out initially and then delete the line after a couple of weeks.}
 -->
 
<!-- If you put each sentence on a new line, it makes it a lot easier for git to spot changes that only affect one line and not the whole paragraph. This reduces merge conflict probability -->

# Introduction and Background

<!-- + Why Graphics? (communication to the public, technological advances, need for research on graphics) -->

\er{
Graphics are a useful tool for displaying and communicating information. 
Researchers include graphics to communicate their results in scientific publications and news sources rely on graphics to convey news stories to the public. 
During the onset of the novel coronavirus - covid19 - pandemic, we saw an influx of dashboards being developed to display case counts, transmission rates, and outbreak regions \citep{lisa_charlotte_2020}.
As a result, people began subscribing to news sources involved in graphically tracking the coronavirus (example John Burn-Murdoch Financial Times - CITE THIS) and gaining more exposure to the use of graphics. 
Many of these graphics helped guide decision makers to implement policies such as shut-downs or mandated mask wearing. 
Better software has meant easier and more flexible drawing, consistent themes, and higher standards.
Consequentially, we must develop a set of principles to help us actively choose which of many possible graphics to draw \citep{unwin_why_2020}.
}
<!--
\er{
As a consequence, principles are needed on how to decide which of many possible graphics to draw \citep{unwin_why_2020}.
}
\svp{
It's probably worth getting rid of passive voice here - we're making active decisions. Something like "Consequentially, we must actively choose which of many possible graphics to draw". I know technical writing classes often prefer passive voice to "take yourself out" of the science, but I think it de-emphasizes the conscious choices we make when we create charts.
}
-->

<!-- + Introduce Log Scales (what are they used for, where are they used (ecological data, covid, etc.)) -->

<!--
\svp{
Start with why we'd choose a log scale: When faced with data which spans several orders of magnitude, we must decide whether to show the data on its original scale (compressing the smaller magnitudes into relatively little area) or to transform the scale and alter the contextual appearance of the data. You could even show a simple example, if you wanted to
}
-->
\er{
When faced with data which spans several orders of magnitude, we must decide whether to show the data on its original scale (compressing the smaller magnitudes into relatively little area) or to transform the scale and alter the contextual appearance of the data. [EXAMPLE HERE]
One common graphical display choice is the use of log scales used to display data over several orders of magnitude within one graph. 
Logarithms convert multiplicative relationships to additive ones, providing an elegant way to span many orders of magnitude, to show elasticities and other proportional changes, and to linearize power laws \citep{menge_logarithmic_2018}. 
When presenting log-scaled data, it is possible to use either untransformed values (for example, values of 1, 10 and 100 are equally spaced along the axis) or log-transformed values (for example, 0, 1, and 2). 
We have recently experienced the benefits and pitfalls of using log-scales as covid-19 dashboards displayed case count data on both the log and linear scale \citep{wade_fagen_ulmschneider_2020}. 
INSERT BENEFITS AND PITFALLS OF LOG SCALES HERE. 
While COVID-19 is the most well known example, log-scales have been used to display data in ecological research, etc. 
PUT OTHER AREAS HERE.
}

<!-- + Previous exponential (log/linear scale) studies (literature review). -->

\er{
Previous research suggests our perception and mapping of numbers to a numberline proceeds logrithmically at first and transitions to linear later in development. 
The logarithmic mapping is more noticible in larger numbers as the transition to linear mapping occurs first for small numbers in young children and later for larger numbers \citep{varshney_why_2013, siegler_numerical_2017, dehaeneLogLinearDistinct2008}.
Early studies explored the estimation and prediction of exponential growth. 
Findings indicate that growth is underestimated when presented both numerically and graphically but that numerical estimation is more accurate than graphical estimation for exponential curves. 
While prior contextual knowledge or experience with exponential growth does not improve estimation, previous instruction on exponential growth reduces the underestimation by adjusting the initial starting value but not adjusting their perception of growth parameter
\citep{wagenaar_misperception_1975, jones_polynomial_1977}.
Estimation was shown to improve when subjects were presented with decreasing exponential functions \citep{timmers_inverse_1977}.
\cite{jones_polynomial_1977}, \cite{wagenaar_extrapolation_1978}, and \cite{jones_generalized_1979} propose competing polynomial models for the perception and extrapolation of exponential series.
We hypothesize that estimation is a two-stage process. First, we identify the type of curve and direction and then use that information for prediction \citep{best_perception_2007}.
}

\er{
This paper aims to investigate the use of log scales to display exponentially increasing data. We hypothesize log scales should make it much easier to estimate the growth paramter since we estimate slopes relatively accurately, resolving much of the difficulty with exponential estimation \citep{mosteller_eye_1981}.
In \cite{menge_logarithmic_2018}, ecologists were surveyed to determine how often ecologists encounter log-scaled data and how well ecologists understand log-scaled data. 
Participants were presented two relationships displayed on linear-linear scales, log-log scales with untransformed values, or log–log scales with log-transformed values. 
\cite{menge_logarithmic_2018} propose three types of misconceptions participants encountered when presented data on log-log scales: 'hand-hold fallacy', 'Zeno's zero fallacy', and 'watch out for curves fallacies'. 
}
<!--
Three falacy's (explain further?)

Two relationships were shown on each graph, with distance from the edge of a habitat on the horizontal axis and population size.

whether each population was increasing or decreasing with distance
how steeply the rabbit population changed with distance compared to the chipmunk population.!!!!
Misconception: ‘hand-hold fallacy’
Climbing something steep is harder than climbing something gradual, but climbing something smooth (such as a window) is harder than climbing something with hand-holds (such as a ladder), regardless of slope.

population level at the edge of the habitat.
Misconception: ‘Zeno’s zero fallacy’
Zeno’s famous paradox states that a distance can never be reduced to zero because it must be halved infinitely many times and it is impossible to perform an infinite number of tasks.

the manner in which a population changes.
3 Misconceptions: ‘watch out for curves fallacies’
all lines in log–log space are also lines in linear–linear space
all lines in log–log space curve upwards in linear–linear space
curves in log–log space have similar curvature in linear–linear space
-->

<!-- + Visual Inference (what is it? how do we use it? etc.) *needs lots of work yet* -->

\er{Recent graphical experiments have utilized statistical lineups to quantify the perception of graphical design choices\citep{vanderplas_clusters_2017}. 
Statistical lineups provide an elegant way of combining perception and statistical signficiance by valdiating the findings from a graphical experiment \citep{buja_statistical_2009, wickham2010graphical, hofmann_graphical_2012, majumder_validation_2013, vanderplas_clusters_2017}.
'Lineups' are named after the 'police lineup' of criminal investigations where witnesses are asked to identify the criminal from a set of individuals. 
Similarly, a statistical lineup is a plot consisting of smaller panels of plots in which the viewer is asked to identify the plot of the real data from among a set of decoys, the null plots. 
A statistical lineup typically consists of 20 panels - 1 target panel and 19 null panels (INSERT EXAMPLE). 
If the viewer can identify the target panel embeded within the set of null panels, this suggests that the real data is interesting or has unqiue properties.
Crowd sourcing websites such as Amazon Mechanical Turk, Reddit, and Proflic allow us to collect responses from multiple viewers.
\cite{vanderplas_statistical_nodate} provides an approach for calculating visual p-values utilizing a 'rorschach' lineup which consists soley of null panels.
}
    
<!-- + What is new in this paper. -->

\er{
In this paper, we use statistical lineups to test human subjects ability to differentiate between increasing exponential data with differing growth rates displayed on both the linear scale and log scale.
}

# Data Generation

\er{
The most common type of lineup used in graphical experiments is a standard lineup containing one "target" dataset embeded within a set of null datasets. 
One way to generate the null datasets when working with real data is through the use of permutation. 
In this study, both the target and null datasets were generated by simulating data from an exponential model with differing parameters. 
This experiment was designed to test a participants ability to differentiate between different rates of exponential growth on both the log and linear scales. 
In order to gaurantee the simulated data spans the same range of values, we implemented a range constraint of $y\in [10,100]$ and a domain constraint of $x\in [0,20]$ with $N = 50$ points randomly assigned throughout the domain and mapped to the y-axis using the exponential model with the selected paramters. 
These constraints provide some assurance that participants who select the target plot are doing so because of their visual perception differentiating between curvature or slope rather than different starting or ending values. 
}

## Exponential Model
\er{
We simulated data based on a 3-parameter exponential model with multiplicative errors. 
This model has a $\beta$ parameter to reflect the rate of growth and amount of curvature and $\sigma^2$ to reflect the amount of variability around the exponential growth curve. 
The parameters $\alpha$ and $\theta$ are adjusted based on $\beta$ and $\sigma^2$ to gauranee the range and domain constraints are met. 
The model generated $N = 50$ points $(x_i, y_i), i = 1,...,N$ where $x$ and $y$ have an increasing exponential relationship. 
The data was simulated heuristically by the following procedures:
}
\vspace{3 mm}

\er{\textit{Algorithm 2.1.1: Paremeter Estimation}}

\er{Input Parameters: domain $x\in[0,20]$, range $y\in[10,100]$, midpoint $x_{mid}$.}

\er{Output: estimated model parameters $\hat\alpha, \hat\beta, \hat\theta$}

1. \er{Determine the $y=-x$ line scaled to fit the assigned domain and range.}

2. \er{Map the values $x_{mid} - 0.1$ and $x_{mid} + 0.1$ to the $y=-x$ line for two additional points.}

3. \er{From the set points $(x_k, y_k)$ for $k = 1,2,3,4$, obtain the coefficients from the linear model $\ln(y_k) = b_0 +b_1x_k$ to obtain starting values - $\alpha_0 = e^{b_0}, \beta_0 =  b_1, \theta_0 = 0.5\cdot \min(y)$}

4. \er{Using the `nls()` function from the `stats` package in Rstudio and the starting parameter values - $\alpha_0, \beta_0, \theta_0$ - fit the nonlinear model, $y_k = \alpha\cdot e^{\beta\cdot x_k}+\theta$ to obtain estimated parameter values - $\hat\alpha, \hat\beta, \hat\theta.$}

\er{\textit{Alogrithm 2.1.2: Exponential Simulation}}

\er{Input Paremeters: sample size $N = 50$, estimated parameters $\hat\alpha$, $\hat\beta$, and $\hat\theta$, $\sigma$ standard deviation from the exponential curve.}

\er{Output Parameters: $N$ points, in the form of vectors $\mathbf{x}$ and $\mathbf{y}$}

1. \er{Generate $\tilde x_j, j = 1,..., N\cdot \frac{3}{4}$ as a sequence of evenly spaced points in $[0,20]$. This ensures the full domain of $x$ is used, fulfilling the constraints of spanning the same domain and range for each parameter combination.}

2. \er{Obtain $\tilde x_i, i = 1,...N$ by sampling $N = 50$ values from the set of $\tilde x_j$ values. This gaurantees some variability and potential clustring in the exponential growth curve disrupting the perception due to continuity of points.} 

3. \er{Obtain the final $x_i$ values by jittering $\tilde x_i$}.

4. \er{Calculate $\tilde\alpha = \frac{\hat\alpha}{e^{\sigma^2/2}}.$ This ensures that the range of simulated values for different standard devaition parameters has an equal expected value for a given rate of change due to the non-constant variance across the domain.}

4. \er{Generate $y_i = \tilde\alpha\cdot e^{\hat\beta x_i + e_i}+\hat\theta$ where $e_i\sim N(0,\sigma^2).$}

## Parameter Selection

\er{
The exponential model provides the base for this graphical experiment. 
We manipulate the midpoint, $x_{mid}$, and in turn the estimated parameters to control the amount of curvature present in the data and the error standard deviation, $\sigma$, to control the amount of deviation from the exponential curve.
We selected three midpoints corresponding to difficulty levels easy (obvious curvature), medium (noticable curvature), and hard (almost linear) along with a sensible choice of standard deviation, $\sigma.$ (INSERT TABLE WITH PARAMETER SELECTIONS HERE). 
The midpoints and standard deviation combinations were chosen similar to in \cite{vanderplas_clusters_2017}. 
For each level of difficulty, we simulated 1000 datasets of $(x_{ij}, y_{ij})$ points for $i = 1,...,50$ and $j = 1...10$. 
Each generated $x_i$ point from \textit{Algorithm 2.1.2} was replicated 10 times.  
Then the lack of fit statistic (LOF) was computed for each simulated dataset by calculating the deviation of the data from a linear line. 
Plotting the density curves of the LOF statistics for each level of difficulty choice allows us to evalute the ability of differentiating between the difficulty levels and thus detecting the target plot.
In figure [INSERT DENSITY CURVE HERE], we can see the densities of each of the three difficulty levels. 
While the LOF statistic provides us a numerical value for discriminating between the difficulty levels, we cannot directly realte this to the perceptual discriminability. 
}

## Lineup Setup
\er{There were a total of three parameter combinations corresponding to the three difficulty levels - easy (obvious curvature), medium (noticable curvature), and hard (almost linear). 
The lineup plots were generated by mapping simulating data corresponding to difficulty level A to a scatterplot while the null plots were generated by mapping simulated data corresponding to difficulty level B to a scatterplot. 
For exmaple, a target plot with simulated data following an increasing exponential curve with obvious curvature is embeded within null plots with simulated data following an increasing exponential that is almost linear (i.e. Easy-Hard). 
By our constraints, the target plot and null plots will span a similar domain and range. 
There are a total of 6 (i.e. 3 choose 2) lineup parameter combinations.
Two sets of each lineup parameter combination were simulated (total of 12 test datasets) and plotted on both the linear and the log scale (total of 24 test lineup plots). 
It is worth noting that there were also three rorschach lineup parameter combinations (e.g. Easy-Easy) and that each of these also had two sets of datasets simulated and plotted on both scales (12 rorscahch lineup plots).
}

## Study Design
\er{
Each participant was shown a total of thirteen lineup plots (twelve) test lineup plots and one rorschach lineup plot). Participants were randomly assigned one of the two replicate datasets for each of the six unique lineup parameter combinations. For each assigned test dataset, the participant was shown the lineup plot corresponding to both the linear scale and the log scale. For the additional rorschach lineup plot, participants were randomly assigned a rorschach dataset on either the linear or the log scale. The order of the thirteen lineup plots shown was randomized for each participant. 
}

## Participant Recruitment

\er{Participants above age 19 were recruited from Reddit's R Visualization community. 
Since participants recruitted on Reddit were not compensated for their time, most participants have an interest in data visualization research. 
Previous literature suggests that prior mathematical knowledge or experience with exponential data is not associated with the outcome of graphical experiments (SITE THIS!). 
Participants were then directed to complete the experimental task available at LINK TO SHINY APP HERE.
}

## Task Description

\er{Participants were shown a series of twelve test lineup plots and asked to identify the plot that was most different from the others. 
On each plot, participants were asked to justify their choice and provide their level of confidence in their choice.
The goal of this experimental task is to test an individuals ability to perceptually differentiate exponentially increasing data with differing rates of change on both the linear and log scale. 
In \cite{best_perception_2007}, the authors explored whether descrimination between curve types is possible. 
They found that accuracy higher when nonlinear trends presented (e.g. it’s hard to say something is linear, but easy to say that it isn’t) and that accuracy higher with low additive variability.
}

# Results
\er{
Participant data was anlalyzed using the Glimmix Procedure in SAS 9.4. Each lineup plot evaluated was assigned a value based on the participant response (correct = 1, not correct = 0). The binary response was analyzed using generalized linear mixed model following a binomial distribution. 
}

## Curvature Differentiation


## Linear vs Log


## Participant Reasoning


# Discussion and Conclusion


# Future Research

<!-- + What we learned from lineups but what we still want to learn. -->
\er{
In this study, we discovered that differentiation between data following exponentially increasing trends with differing growth rates is FINISH THIS.
}

<!-- + You draw it -->

\er{
Further experimentation is necessary to test an individual's ability to make predictions for exponentially increasing data. 
Previous literature suggests that we tend to underestimate predictions of exponentially increasing data.\citep{jones_generalized_1979, jones_polynomial_1977, wagenaar_extrapolation_1978}.
\citep{mosteller_eye_1981} designed and carried out an empirical investigation to explore properties of lines fitted by eye. 
The researchers found that students tended to fit the slope of the first principal component or major axis (the line that minimizes the sum of squares of perpendicular rather than vertical distances) and that students who gave steep slopes for one data set also tended to give steep slopes on the others. 
Interestingly, the individual-to-individual variability in slope and in intercept was near the standard error provided by least squares.
A similar graphical taxk is used in the New York Times "You Draw It" page asking readers to test their knowledge by using their curser to estimate values of a certain topic under different political administrations or over different years (CITE THIS).
}

<!-- + Estimation -->

\er{
In addition to differentiation and prediction of exponentially increasing data, it is of interest to test an individuals ability to translate a graph of exponentially increasing data into real value quantities and extend their estimations by making comparisons. 
\citep{friel_making_2001} emphasize the importance of graph comprehension proposing that the graph construction plays a role in the ability to read and interpret graphs.
}
  
# Supplementary Materials {-}

<!-- # Acknowledgement(s) {-} -->

# References
